# CS231n_assignment2020
CS231n: Convolutional Neural Networks for Visual Recognition.

# 用做笔记 知识点&小记录

## 2020.7.1

### Lecture-1
    计算机视觉概述，历史背景
### Lecture-2
    K近邻，线性分类I
    L1 distance:曼哈顿距离（对应点绝对值差之和）     适用于明确含义值
    L2 distance:D2(I1,I2)=sqr(sum((i1-i2)**2))     适用于几何距离
### Lecture-3
    线性分类II，高级表示，图像特征优化，随机梯度下降
    支持向量机：铰链损失函数Multi-SVM Loss:分类错误的指标与正确得分差值，与0的较大者累加 L=sum（max（0，sj-si+1))   当正确类别分数较高且显著时，
        这个损失函数会趋于0，是一个校验损失函数，惩罚分类易混淆的类得分。
    L1 正则：权重绝对值相加
    L2 正则：权重平方相加  
    Lamda过大时惩罚过度造成权重归零，过小时惩罚太轻造成过拟合。即惩罚大时训练Loss会增大，但是可能增强模型实用性。
    SoftMax classifier: 先求exp，再归一化计算分类概率
    交叉熵损失函数/负对数损失函数：Loss=-log(P)  P为类预测概率 y=logx为递减且属于（-无穷，0），所以加一个负号翻转为正  
        目的：将多类别概率相乘简化为对数相加，Loss越少说明预测效果越好，正确类别越突出。
    梯度下降：解析解W_grad = eval_grad(loss, data, weights) 反向传播更新权重。
    SGD随机梯度下降容易陷入局部最优。
    动量下降会有惯性。
    更新权重时，全部数据每轮迭代很不经济，所以引出mini-Batch批的概念，小碎步快速迈进。
    特征工程：提取图片特征，用新的高维向量代替图片用来进行分类训练，提高分类效率和效果。由卷积神经网络完成这部分工作，卷积核决定的特征提取。
        优势：特征表达性更好、运算量得以减少、语义特征得到更大重视。
    特征包括：颜色通道、直方图特征、BoW、等
## 2020.7.2
### Lecture-4
    传统视觉方法：HOG、SIFT等特征提取
    深度学习方法：大数据支持下的端到端自提取特征
    全连接神经网络FCN多层感知机MLP
    激活函数带来非线性，否则多层的权重W1W2W3相乘可用一个W'代替，即与单层感知机无异。
    反向传播 局部求导梯度   加法梯度复制，乘法梯度交换，Max梯度路由只走权重最大
    雅可比矩阵：稀疏矩阵，仅在对角线有非零值的矩阵。
    隐含层将非线性数据转换为线性线性可分数据，在最后进行线性分类。一个神经元可看作一个线性边界。
    单层感知机在神经元个数足够时可以拟合任何问题，但是宽泛且浅，运算上不经济。通过添加层数可以更好发挥激活函数的作用。
## 2020.7.3
### Lecture-5
    卷积神经网络
    权值共享 卷积核共享
    padding
    输入通道与卷积核通道相同
    feature map
    多通道卷积后求和为输出
    Output_Size = （N + 2P -F )/S + 1
    1*1 conv的作用：1、降维或升维 2、跨通道信息交融 3、减少参数量 4、增加模型深度，提高非线性表示能力
    趋势：卷积核减小（运算量），深度加深（更好表达提取特征）
## 2020.7.5
### Lecture-6
    #可视化CNN为什么work
    普通反向传播：常规回传梯度；  导向反向传播：激活值与梯度都为正才会回传梯度
### Lecture-7
    开始训练网络。
    固定参数与结构设计：激活函数、网络结构
    数据预处理：减少计算量，增加可表达性
    权重初始化：避免初始化为同一参数，
    Xavier初始化：'w = np.random.randn(Din, Dout) / np.sqrt(Din) '维度越多，初始化幅度值越小--标准正态分布
    Kaiming初始化：'w = np.random.randn(Din, Dout) / np.sqrt(2 / Din) '
    BN Batch_Normalization：   --防止过拟合，加快收敛速度
        一个batch中有N个数据，每个数据D维；
        1、求出每个特征维度的均值得到D个均值U_i 
        2、求出每一个特征维度的方差得到D个方差Delta_i
        3、对batch中每一个数据进行BN 得到X(i,j)   --shape(N*D)
        4、Output = Gama * X(i,j) + Beta   --shape(N*D)
        When Test:全体均值代替mini-batch均值，全体方差代替mini-batch方差
## 2020.7.7
### Lecture-7
    Optimizers 梯度下降优化器、LR Schedules 学习率下降策略、Regularization 正则化、Dropout、Hyperparameters 超参数选择
    Optimizers 梯度下降优化器：
        SGD x = - LR * dx
        Momentum  X+1 = - LR * (belta * X +dx)  起到梯度平滑的作用，避免过大震荡
        AdaGrad x = - LR * dx / (np.sqrt(dx * dx) + 1e-7)  常数项避免分母为零， ---惩罚过大斜率  np.sqrt-求和开根号
    学习率选择：
        学习率函数下降方法
    防止过拟合：
        多个模型好而不同，Regularization 正则化、Dropout
    DropOut:
        简化理解为多个结构不同的2的N次方个模型集成，且之间权重共享
        训练时间加大，因为掐死一部分后完成整体训练需要更多轮次
    数据增强：
        旋转、裁剪、遮掩、拉伸、模糊、颜色变换
    超参数选择：
        机器算法智能选择



